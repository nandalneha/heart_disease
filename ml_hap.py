# -*- coding: utf-8 -*-
"""ML-HAP.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1-QlyYtQTPjdKaT8aFot2RO4R1byEgeHI
"""

import numpy as np # linear algebra
import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)

# Input data files are available in the read-only "../input/" directory
# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory

import os
for dirname, _, filenames in os.walk('/kaggle/input'):
    for filename in filenames:
        print(os.path.join(dirname, filename))



import numpy as np
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.metrics import roc_curve, roc_auc_score
from sklearn.metrics import confusion_matrix ,classification_report,precision_score, recall_score ,f1_score 

from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression  
from sklearn.neighbors import KNeighborsClassifier
from sklearn.naive_bayes import GaussianNB
from sklearn.svm import SVC 
import warnings
warnings.filterwarnings('ignore')

data = pd.read_csv('heart.csv')

data.head()

data.info()

data.describe()

data.describe().columns

data_num = data[['age','trtbps','chol','thalachh','oldpeak']]
data_cat =data[['sex','cp','fbs','restecg','exng']]

for i in data_num.columns:
    plt.hist(data_num[i])
    plt.title(i)
    plt.show()

pd.pivot_table(data, index='output', values=['age','trtbps','chol','thalachh','oldpeak'])

for i in data_cat.columns:
    sns.barplot(data_cat[i].value_counts().index,data_cat[i].value_counts()).set_title(i)
    plt.show()

print(pd.pivot_table(data,index='output',columns='sex', values='age'))
print("="*100)
print(pd.pivot_table(data,index='output',columns='cp', values='age'))
print("="*100)
print(pd.pivot_table(data,index='output',columns='fbs', values='age'))
print("="*100)
print(pd.pivot_table(data,index='output',columns='restecg', values='age'))
print("="*100)
print(pd.pivot_table(data,index='output',columns='exng', values='age'))

print(pd.pivot_table(data,index='output',columns='sex', values='chol'))
print("="*100)
print(pd.pivot_table(data,index='output',columns='cp', values='chol'))
print("="*100)
print(pd.pivot_table(data,index='output',columns='fbs', values='chol'))
print("="*100)
print(pd.pivot_table(data,index='output',columns='restecg', values='chol'))
print("="*100)
print(pd.pivot_table(data,index='output',columns='exng', values='chol'))

for i in data_num.columns:
    sns.boxplot(data_num[i])
    plt.title(i)
    plt.show()

def outlinefree(dataCol):
    # sorting column
    sorted(dataCol)
    
    # getting percentile 25 and 27 that will help us for getting IQR (interquartile range)
    Q1,Q3 = np.percentile(dataCol,[25,75])
    
    # getting IQR (interquartile range)
    IQR = Q3-Q1
    
    # getting Lower range error
    LowerRange = Q1-(1.5 * IQR)
    
    # getting upper range error
    UpperRange = Q3+(1.5 * IQR)
    
    # return Lower range and upper range.
    return LowerRange,UpperRange

lwtrtbps,uptrtbps = outlinefree(data['trtbps'])
lwchol,upchol = outlinefree(data['chol'])
lwoldpeak,upoldpeak = outlinefree(data['oldpeak'])

data['trtbps'].replace(list(data[data['trtbps'] > uptrtbps].trtbps) ,uptrtbps,inplace=True)
data['chol'].replace(list(data[data['chol'] > upchol].chol) ,upchol,inplace=True)
data['oldpeak'].replace(list(data[data['oldpeak'] > upoldpeak].oldpeak) ,upoldpeak,inplace=True)

features = data.iloc[:,:-1].values
label = data.iloc[:,-1].values

#------------------------LogisticRegression-----------------------
X_train, X_test, y_train, y_test= train_test_split(features,label, test_size= 0.25, random_state=102)

classimodel= LogisticRegression()  
classimodel.fit(X_train, y_train)
trainscore =  classimodel.score(X_train,y_train)
testscore =  classimodel.score(X_test,y_test)  

print("test score: {} train score: {}".format(testscore,trainscore),'\n')

y_pred =  classimodel.predict(X_test)

#from sklearn.metrics import confusion_matrix
confusion_matrix(y_test, y_pred)

print(' f1 score: ',f1_score(y_test, y_pred),'\n')
print(' precision score: ',precision_score(y_test, y_pred),'\n')
print(' recall score: ',recall_score(y_test, y_pred),'\n')
print(classification_report(y_test, y_pred))

#--------------------------------------K-Nearest Neighbor(KNN)-----------------
X_train, X_test, y_train, y_test= train_test_split(features,label, test_size= 0.25, random_state=193) 


classifier= KNeighborsClassifier()  
knnmodel =  classifier.fit(X_train, y_train) 

trainscore =  knnmodel.score(X_train,y_train)
testscore =  knnmodel.score(X_test,y_test)  

print("test score: {} train score: {}".format(testscore,trainscore),'\n')

y_predknn =  knnmodel.predict(X_test)

print(confusion_matrix(y_test, y_predknn))

print("f1_score: ",f1_score(y_test, y_predknn),'\n')
print("precision_score: ",precision_score(y_test, y_predknn),'\n')
print("recall_score: ",recall_score(y_test, y_predknn),'\n')
print(classification_report(y_test, y_predknn))

#------------------------------naive bayes---------------------------
X_train, X_test, y_train, y_test= train_test_split(features,label, test_size= 0.25, random_state=34) 

NBmodel = GaussianNB()  
NBmodel.fit(X_train, y_train) 

trainscore =  NBmodel.score(X_train,y_train)
testscore =  NBmodel.score(X_test,y_test)  

print("test score: {} train score: {}".format(testscore,trainscore),'\n')
y_predNB =  NBmodel.predict(X_test)
print(confusion_matrix(y_test, y_predNB))

print("f1_score: ",f1_score(y_test, y_predNB),'\n')
print("precision_score: ",precision_score(y_test, y_predNB),'\n')
print("recall_score: ",recall_score(y_test, y_predNB),'\n')
print(classification_report(y_test, y_predNB))

#-------------------------------- support vector classification -------------------------------------  
X_train, X_test, y_train, y_test= train_test_split(features,label, test_size= 0.25, random_state=8) 

svcmodel = SVC(probability=True)  
svcmodel.fit(X_train, y_train) 

trainscore =  svcmodel.score(X_train,y_train)
testscore =  svcmodel.score(X_test,y_test)  

print("test score: {} train score: {}".format(testscore,trainscore),'\n')

y_predsvc =  svcmodel.predict(X_test)

print(confusion_matrix(y_test, y_predsvc))

print("f1_score: ",f1_score(y_test, y_predsvc),'\n')
print("precision_score: ",precision_score(y_test, y_predsvc),'\n')
print("recall_score: ",recall_score(y_test, y_predsvc),'\n')
print(classification_report(y_test, y_predsvc),'\n')

#-------------------------------------- LogisticRegression -------------------------------------
probabilityValues = classimodel.predict_proba(features)[:,1]
#Calculate AUC
auc = roc_auc_score(label,probabilityValues)
print(auc)
#Calculate roc_curve
fpr,tpr, threshold =  roc_curve(label,probabilityValues)
plt.plot([0,1],[0,1], linestyle = '--')
plt.plot(fpr,tpr)

#-------------------------------------- KNeighborsClassifier -------------------------------------
probabilityValues = knnmodel.predict_proba(features)[:,1]
#Calculate AUC
auc = roc_auc_score(label,probabilityValues)
print(auc)
#Calculate roc_curve
fpr,tpr, threshold =  roc_curve(label,probabilityValues)
plt.plot([0,1],[0,1], linestyle = '--')
plt.plot(fpr,tpr)

#-------------------------------------- naive bayes -------------------------------------
probabilityValues = NBmodel.predict_proba(features)[:,1]
#Calculate AUC
auc = roc_auc_score(label,probabilityValues)
print(auc)
#Calculate roc_curve
fpr,tpr, threshold =  roc_curve(label,probabilityValues)
plt.plot([0,1],[0,1], linestyle = '--')
plt.plot(fpr,tpr)

#-------------------------------------- SVC -------------------------------------
probabilityValues = svcmodel.predict_proba(features)[:,1]
#Calculate AUC
auc = roc_auc_score(label,probabilityValues)
print(auc)
#Calculate roc_curve
fpr,tpr, threshold =  roc_curve(label,probabilityValues)
plt.plot([0,1],[0,1], linestyle = '--')
plt.plot(fpr,tpr)

import xgboost as xgb
from sklearn.metrics import mean_squared_error
import pandas as pd
import numpy as np

X_train, X_test, y_train, y_test= train_test_split(features,label, test_size= 0.25, random_state=102)

XGmodel= xgb.XGBRFClassifier()
XGmodel.fit(X_train, y_train)
trainscore =  XGmodel.score(X_train,y_train)
testscore =  XGmodel.score(X_test,y_test)  

print("test score: {} train score: {}".format(testscore,trainscore),'\n')

y_predXG =  XGmodel.predict(X_test)

#from sklearn.metrics import confusion_matrix
confusion_matrix(y_test, y_pred)

print("f1_score: ",f1_score(y_test, y_predXG),'\n')
print("precision_score: ",precision_score(y_test, y_predXG),'\n')
print("recall_score: ",recall_score(y_test, y_predXG),'\n')
print(classification_report(y_test, y_predXG),'\n')

#-------------------------------------- SVC -------------------------------------
probabilityValues = XGmodel.predict_proba(features)[:,1]
#Calculate AUC
auc = roc_auc_score(label,probabilityValues)
print(auc)
#Calculate roc_curve
fpr,tpr, threshold =  roc_curve(label,probabilityValues)
plt.plot([0,1],[0,1], linestyle = '--')
plt.plot(fpr,tpr)



















